# -*- coding: utf-8 -*-
# Author:Tom Chen
# date:2019/05/01
import requests
import re
import matplotlib as plt
import importlib
import pdb
import os
import sys
import csv
import time
import json
import winsound
import multiprocessing
sys.getdefaultencoding()  #默认编码是“utf-8”
"""
这是一个用于爬取学科网各年级各学科各种材料信息的定向爬虫
旨在为一线的老师能够根据数据关于下载量，阅读量及价格的分析对比选出最优材料
"""

class SpiderXueKeWang(object):
    
    # 学科及材料对应域参数
    def __init__(self):
        self.headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; \
			Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/7\
			1.0.3578.98 Safari/537.36'}
        self.courses_lst_primary = {"数学":"sx","英语":"yy","科学":"kx",
        "美术":"ms","音乐":"yinyue","品德":"pd","信息技术":"xx","体育":"ty"}
        self.courses_lst_middle = {"语文":"yw","数学":"sx","英语":"yy",
        "物理":"wl","化学":"hx","生物":"sw","政治":"zz","历史":"ls",
        "地理":"dl","综合":"zh","科学":"kx"}
        self.courses_lst_high = {"语文":"yw","数学":"sx","英语":"yy",
        "物理":"wl","化学":"hx","生物":"sw","政治":"zz","历史":"ls",
        "地理":"dl","综合":"zh","通用":"qt"}
        self.sources_kinds_primary = {"课件":"type1","教案":"type2",
        "试题":"type3","同步扩展":"type4","素材资料":"type5","视频":"type30"}
        self.sources_kinds_middle_high = {"试题试卷":"type1","教案":"type2",
        "课件":"type3","素材":"type4","视频":"type6","备课综合":"type7",
        "学案/导学案":"type8","作业":"type9"}
        self.grades_dict = {"小学":[self.courses_lst_primary,self.sources_kinds_primary],
                           "初中":[self.courses_lst_middle,self.sources_kinds_middle_high],
                           "高中":[self.courses_lst_high,self.sources_kinds_middle_high]
                          }
        self.s = requests.Session()
        # 付费代理API地址抽取
        self.__API = API 
        self.time = 1
        self.duration = 1000  # 警报器发声时间
        self.freq = 440  # 警报器提示音频率Hz
    
    
    # 构造爬取目标网址
    def url_parse(self,grade,course,source_type,page):
        if grade == "小学":
            url_grade = f"http://www.xuekeedu.com/softlist/{course}-{source_type}/index-{page}.html"
        elif grade == "初中":
            url_grade = f"http://middle.school.zxxk.com/jc/{course}-{source_type}/index-{page}.html"
        else:
            url_grade = f"http://high.school.zxxk.com/jc/{course}-{source_type}/index-{page}.html"
        return url_grade
        
        
    # 代理地址抽取
    def proxies_initial_pick(self):
        API = """由于学科网暂时没有封ip，所以此模块没有用，等有需要的时候，这里就
		加上你的付费代理抽取模块就行了"""
        
    # csv文件存储模块   
    def csv_writer(self,filepath,information_generator):
        print(f"正在写入{filepath}")
        with open(f"{filepath}\data.csv","a",encoding = "UTF-8",newline = "") as f:
            writer = csv.writer(f)
            for information in information_generator:
                writer.writerow(information)
        
        
    # 存储文件夹创建模块    
    def mike_dir_file_box(self,grade,filepath):
        try:
            os.makedirs(filepath)
            with open(f"{filepath}\data.csv","a",encoding = "UTF-8",newline = "") as f:
                writer = csv.writer(f)
                if grade == "小学":
                    writer.writerow(["课件下载地址","课件标题","上传时间","类型","大小","下载量","作者个人主页","作者","资源类型"])
                else:
                    writer.writerow(["课件下载地址","课件标题","资源花销","是否热点","是否推荐","上传时间","材料大小","下载量"])
        except FileExistsError as f:
            pass
    
    
    # 数据解析
    def data_get_parse(self,grade,html_text):
        pattern_primary = re.compile(r"""(
        <a\s+class="softTitle"\s+href="(\S+)"\s+title="(.*?)"                           # 提取下载地址,课件题目
        .*?class="tim">(\d+/\d+/\d+\s+\d+:\d+:\d+)                                      # 提取上传时间
        .*?class="stl">([\u4e00-\u9fa5]+)                                               # 提取文件类型
        .*?class="kb">.*?(\d+)</span>\w+</p>                                            # 提取文件大小
        .*?class="dow">.*?(\d+).*?</p>                                                  # 提取下载量
        .*?class="aut">[\u4e00-\u9fa5]+.*?<a\s{1}href="(.*?)">\S+\s+\S+>(\S+)\s?</span> # 提取作者个人主页,作者名
        .*?<a\s+class=\S{1}tag\s+leavel\d?\S{1}>\s+(\S+|\s+)\s+</a>                     # 提取资源级别
        )""",re.VERBOSE|re.S)
        
        pattern_middle_high = re.compile(r"""(
        class=["|']left["|'].*?<a\s{1}href=["|'](.*?)["|']                                      # 课件下载地址
        .*?title=["|'](.*?)["|']                                                                # 课件题目
        .*?class=\S{1}emdown\S{1}>(.*?)</a>                                                     # 资源花销
        .*?class=\S{1}stares\S+.*?class=\S{1}(\w+\s+\S+)\S{1}></span>                           # 是否是热点
        .*?class=\S{1}(\w+\s+\S+)\S{1}></span>                                                  # 是否是推荐
        .*?class=\S{1}time\S{1}>(\d+/\d+/\d+\s+\d+:\d+:\d+)</span>                              # 上传时间
        .*?class=\S{1}size\S{1}>(\d+\w+)</span>                                                 # 材料大小
        .*?font\s+\w+=\S{1}\w+\d+\S{1}>(\d+)</font>                                             # 下载量
        )""",re.VERBOSE|re.S)
        if grade == "小学":
            result_items = re.findall(pattern_primary,html_text)
        else:
            result_items = re.findall(pattern_middle_high,html_text)
        if not result_items:
            return "没有匹配到对象，请检查正则"
        else:
            information_generator = (i[1:] for i in result_items)
            return information_generator
        
        
            
    
    # 页面最大值获取
    def top_page_get(self,grade,course_en,source_type,page):
        url = self.url_parse(grade,course_en,source_type,page)
        print(url)
        pattern_pagemax_primary = re.compile(r"""(
		class="cur">\d+.*?(\d+)</span>
		)""",re.VERBOSE|re.S)
        pattern_pagemax_middle_high = re.compile(r"""(
		<strong\s+id=\S+articleTotalPage\S+>(\d+)</strong>
		)""",re.VERBOSE|re.S)
        try:
            response = self.s.get(url = url,headers = self.headers,timeout = 5)
            if response.status_code == 200:
                if grade == "小学":
                    result = re.search(pattern_pagemax_primary,response.text).group(2)
                else:
                    result = re.search(pattern_pagemax_middle_high,response.text).group(2)
                return result
            return None
        except Exception as e:
            print(e)
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   
          
# 主程序运行
    def run(self,grade):
        courses_sources = self.grades_dict[grade]
        for course_chn,course_en in courses_sources[0].items():
            for kind,source_type in courses_sources[1].items():
                filepath = f"E:\学科网\{grade}\{course_chn}\{kind}"
                self.mike_dir_file_box(grade,filepath)
                page = 1
                page_max = self.top_page_get(grade,course_en,source_type,page)
                print(f"正在爬取{grade}-{course_chn}-{kind},最大页数为：{page_max}")
                while page <= int(page_max):
                    try:
                        url = self.url_parse(grade,course_en,source_type,page)
                        print(url)
                        response = self.s.get(url = url,headers = self.headers,timeout = 5)
                        if response.status_code == 200:
                            information_generator = self.data_get_parse(grade,response.text)
                            if isinstance(information_generator,str):
                                print(information_generator)
                                # 出现问题提示音响起
                                winsound.Beep(self.freq,self.duration)
                                with open(r"E:\学科网\url_rex_nothing.csv","a",encoding = "UTF-8",newline = "") as f:
                                    writer = csv.writer(f)
                                    writer.writerow(url)
                            else:
                                self.csv_writer(filepath,information_generator)
                                page += 1
                        else:
                            sys.exit()
                    except Exception as e:
                        print(e)
                    
                    
        
if __name__ == "__main__":
    spider_xuekewang = SpiderXueKeWang()
    p = multiprocessing.Pool(3)
    p.map(spider_xuekewang.run, ["小学","初中","高中"])
 
